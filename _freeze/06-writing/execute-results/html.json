{
  "hash": "cf3e2c10688cb5a182310491a62d7cf5",
  "result": {
    "engine": "knitr",
    "markdown": "\n# Writing Code with AI\n\nThe best AI platform for writing code is arguably Github Copilot (or Super Clippy as my programmer friends like to call it). However, unless coding is the main part of your job, most people are unlikely to have this subscription service so we'll stick with the generic platforms.\n\nIf you have access to LinkedIn Learning (which you do if you are a UofG student or staff), I'd also highly recommend the short course [Pair Programming with AI](https://www.linkedin.com/learning/pair-programming-with-ai/) by Morten Rand-Hendriksen. It only takes 1.5 hours to work through and he covers using both Github Copilot and ChatGPT and has a nicely critical view of AI.\n\nIf you've worked through this entire book then hopefully what you've learned is that AI is very useful but I also hope that you have a healthy mistrust of anything it produces which is why this chapter is the last in the coding section. \n\nThe number 1 rule to keep in mind when using AI to write code is that the platforms we're using **are not intelligent**. They are not thinking. They are not reasoning. They are not telling the truth or lying to you. They don't have gaps in their knowledge because they have no knowledge. They are digital parrots on cocaine. \n\n## Activity 1: Set-up\n\nAgain we'll use the `palmerpenguins` dataset for this exercise so open up an Rmd and run the following:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(palmerpenguins)\ndata(\"penguins\")\n```\n:::\n\n\nWe'll also do a little bit of set-up for the AI:\n\n> Act like an expert programmer in R. I want you to help me write code. The code should be commented and should use the tidyverse where possible. Ask me questions about the code before you write it if necessary.\n\n## Activity 2: Knowledge is power\n\nWhen you input this starting prompt, there's a good chance you'll get something like the following:\n\n\n::: {.cell}\n::: {.cell-output-display}\n![More info needed](include/images/06-writing/write1.png){width=780}\n:::\n:::\n\n\nBefore you ask the AI to write code, it's helpful to give it as much information as you can about your dataset. You could write out a description manually but there's a few options to automate. \n\n`summary()` is useful because it provides a list of all variables with some descriptive statistics so that the AI has a sense of the type and range of data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nsummary(penguins)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n      species          island    bill_length_mm  bill_depth_mm  \n Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  \n Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  \n Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  \n                                 Mean   :43.92   Mean   :17.15  \n                                 3rd Qu.:48.50   3rd Qu.:18.70  \n                                 Max.   :59.60   Max.   :21.50  \n                                 NA's   :2       NA's   :2      \n flipper_length_mm  body_mass_g       sex           year     \n Min.   :172.0     Min.   :2700   female:165   Min.   :2007  \n 1st Qu.:190.0     1st Qu.:3550   male  :168   1st Qu.:2007  \n Median :197.0     Median :4050   NA's  : 11   Median :2008  \n Mean   :200.9     Mean   :4202                Mean   :2008  \n 3rd Qu.:213.0     3rd Qu.:4750                3rd Qu.:2009  \n Max.   :231.0     Max.   :6300                Max.   :2009  \n NA's   :2         NA's   :2                                 \n```\n\n\n:::\n:::\n\n\n`str()` is also useful because it lists the variables, their data type, and the initial values for each variable. However, that means that you are giving it at least some of the raw data so you have to be very careful if you have sensitive / confidential data and you must ensure that any use of AI is in line with your data management plan. Using Copilot Enterprise means the data won't be stored and used to train the AI further so it's potentially the best option (which is not to say it's safe or problem free, please be careful and critical!).\n\n\n::: {.cell}\n\n```{.r .cell-code}\nstr(penguins)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntibble [344 × 8] (S3: tbl_df/tbl/data.frame)\n $ species          : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ island           : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ bill_length_mm   : num [1:344] 39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ...\n $ bill_depth_mm    : num [1:344] 18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ...\n $ flipper_length_mm: int [1:344] 181 186 195 NA 193 190 181 195 193 190 ...\n $ body_mass_g      : int [1:344] 3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ...\n $ sex              : Factor w/ 2 levels \"female\",\"male\": 2 1 1 NA 1 2 1 2 NA NA ...\n $ year             : int [1:344] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ...\n```\n\n\n:::\n:::\n\n\n\nFinally, `ls()` provides a list of all the variables in a given object. It doesn't provide any info on the variable type or sample, but that does mean it's the most secure and depending on the task, this might be all the info you really need to give the AI. I would suggest starting with `ls()` and only scaling up if necessary (and your data isn't sensitive):\n\n\n::: {.cell}\n\n```{.r .cell-code}\nls(penguins)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n[1] \"bill_depth_mm\"     \"bill_length_mm\"    \"body_mass_g\"      \n[4] \"flipper_length_mm\" \"island\"            \"sex\"              \n[7] \"species\"           \"year\"             \n```\n\n\n:::\n:::\n\n\nIn your AI of choice, input the following:\n\n> here are the variables in my data set\n\n> ls(penguins)\n[1] \"bill_depth_mm\"     \"bill_length_mm\"    \"body_mass_g\"       \"flipper_length_mm\"\n[5] \"island\"            \"sex\"               \"species\"           \"year\" \n\nWithout being asked, Copilot has provided some examples of how you might analyse this data using R and the code all works as intended.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load the tidyverse package\nlibrary(tidyverse)\n\n# Assuming your data is in a dataframe called 'penguins'\n# Let's start by getting a summary of each numerical variable\npenguins %>%\n  summarise(\n    Average_Bill_Depth = mean(bill_depth_mm, na.rm = TRUE),\n    Average_Bill_Length = mean(bill_length_mm, na.rm = TRUE),\n    Average_Body_Mass = mean(body_mass_g, na.rm = TRUE),\n    Average_Flipper_Length = mean(flipper_length_mm, na.rm = TRUE)\n  )\n```\n\n::: {.cell-output-display}\n<div class=\"kable-table\">\n\n| Average_Bill_Depth| Average_Bill_Length| Average_Body_Mass| Average_Flipper_Length|\n|------------------:|-------------------:|-----------------:|----------------------:|\n|           17.15117|            43.92193|          4201.754|               200.9152|\n\n</div>\n:::\n\n```{.r .cell-code}\n# Now, let's see the distribution of species across different islands\npenguins %>%\n  count(island, species) %>%\n  ggplot(aes(x = island, y = n, fill = species)) +\n  geom_bar(stat = \"identity\", position = \"dodge\") +\n  labs(title = \"Distribution of Penguin Species Across Islands\",\n       x = \"Island\",\n       y = \"Count\")\n```\n\n::: {.cell-output-display}\n![](06-writing_files/figure-html/unnamed-chunk-5-1.png){width=672}\n:::\n\n```{.r .cell-code}\n# We can also explore the relationship between flipper length and body mass\npenguins %>%\n  ggplot(aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(aes(color = species)) +\n  geom_smooth(method = \"lm\") +\n  labs(title = \"Relationship Between Flipper Length and Body Mass\",\n       x = \"Flipper Length (mm)\",\n       y = \"Body Mass (g)\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_smooth()`).\n```\n\n\n:::\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 2 rows containing missing values or values outside the scale range\n(`geom_point()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](06-writing_files/figure-html/unnamed-chunk-5-2.png){width=672}\n:::\n\n```{.r .cell-code}\n# If you want to perform more specific analyses or visualizations,\n# please let me know what you're interested in!\n```\n:::\n\n\nI find this hugely problematic. Psychology has spent the last decade dealing with a replication and reproducibility crisis stemming in part because of questionable research practices. As a researcher, you should be making informed decisions as to how you analyse your data and I fear that this type of output is going to increase phishing and atheoretical, exploratory-as-confirmatory nonsense. What happens when the example code the AI spits out without being asked turns out to be a significant regression model that you would never have predicted or run yourself? Are you going to delete it? Or convince yourself that you were going to run it anyway and there's a perfectly logical explanation?\n\nBefore I have a full blown existential crisis, let's get back on track.\n\n## Activity 3: Writing code\n\nFirst, we'll aim to produce a table of descriptive statistics for `body_mass_g` and a histogram. Because it has the exact variable and object names, it should write code we can easily copy and paste:\n\n> Create a table of descriptive statistics for body mass and a histogram of that variable\n\nThis is ChatGPT's output (Copilot was very similar):\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Load the tidyverse package\nlibrary(tidyverse)\n\n# Load the penguins dataset (assuming it's from the palmerpenguins package)\ndata(\"penguins\")\n\n# Check the structure of the dataset\nstr(penguins)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\ntibble [344 × 8] (S3: tbl_df/tbl/data.frame)\n $ species          : Factor w/ 3 levels \"Adelie\",\"Chinstrap\",..: 1 1 1 1 1 1 1 1 1 1 ...\n $ island           : Factor w/ 3 levels \"Biscoe\",\"Dream\",..: 3 3 3 3 3 3 3 3 3 3 ...\n $ bill_length_mm   : num [1:344] 39.1 39.5 40.3 NA 36.7 39.3 38.9 39.2 34.1 42 ...\n $ bill_depth_mm    : num [1:344] 18.7 17.4 18 NA 19.3 20.6 17.8 19.6 18.1 20.2 ...\n $ flipper_length_mm: int [1:344] 181 186 195 NA 193 190 181 195 193 190 ...\n $ body_mass_g      : int [1:344] 3750 3800 3250 NA 3450 3650 3625 4675 3475 4250 ...\n $ sex              : Factor w/ 2 levels \"female\",\"male\": 2 1 1 NA 1 2 1 2 NA NA ...\n $ year             : int [1:344] 2007 2007 2007 2007 2007 2007 2007 2007 2007 2007 ...\n```\n\n\n:::\n\n```{.r .cell-code}\n# Summary statistics for body mass\nbody_mass_summary <- penguins %>%\n  summarise(\n    Mean = mean(body_mass_g, na.rm = TRUE),\n    Median = median(body_mass_g, na.rm = TRUE),\n    SD = sd(body_mass_g, na.rm = TRUE),\n    Min = min(body_mass_g, na.rm = TRUE),\n    Max = max(body_mass_g, na.rm = TRUE)\n  )\n\n# Print the summary statistics\nprint(body_mass_summary)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 5\n   Mean Median    SD   Min   Max\n  <dbl>  <dbl> <dbl> <int> <int>\n1 4202.   4050  802.  2700  6300\n```\n\n\n:::\n\n```{.r .cell-code}\n# Create a histogram of body mass\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_histogram(fill = \"skyblue\", color = \"black\", bins = 20) +\n  labs(\n    title = \"Distribution of Body Mass\",\n    x = \"Body Mass (g)\",\n    y = \"Frequency\"\n  ) +\n  theme_minimal()\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: Removed 2 rows containing non-finite outside the scale range\n(`stat_bin()`).\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](06-writing_files/figure-html/unnamed-chunk-6-1.png){width=672}\n:::\n:::\n\n\nSome notes:\n\n* I didn't have to use the exact variable name `body_mass_g`, it has figured out from my request that's the one I want to use.\n* It's also figured out that we're using the `palmerpenguins` dataset, although it hasn't given us the code to load that package so if we just copied and pasted this into a fresh script without having loaded the data already, it wouldn't work.\n* It's used `na.rm = TRUE` to ignore missing values. This is a very common choice but remember **it is a choice**. You might not want this. \n* Similarly, it's specified `binwidth = 20` for the histogram. Looking at the data this seems like a reasonable binwidth to use, but again, it is a decision the AI has imposed and you must be aware of what you're accepting. \n* I am aware that I am using phrases like \"choice\" and \"decision\" that imbue the AI with consciousness. I know that's not true, I just can't figure out how to write it any other way.\n\n## Activity 4: Giving the AI more freedom\n\nMy initial request to write code was relatively simple and relatively well-specified but let's try an example where there's more room for the AI to make choices.\n\n> Create a new variable that codes each penguin as chonky_boi if they're a large male, chonky_gurl if they're a large female, smol_boi if they're a small male, and smol_boi if they're a small female.\n\nThis prompt gives a lot of leeway. I haven't specified what to call the new variable, what criteria should be used to determine large and small, or what to do with observations that are neither.\n\nChatGPT used percentiles to determine category, however, this code didn't work - it returns all observations as \"average\":\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a new variable based on conditions\npenguins <- penguins %>%\n  mutate(chonkiness = case_when(\n    sex == \"Male\" & body_mass_g > quantile(body_mass_g, 0.75, na.rm = TRUE) ~ \"chonky_boi\",\n    sex == \"Female\" & body_mass_g > quantile(body_mass_g, 0.75, na.rm = TRUE) ~ \"chonky_gurl\",\n    sex == \"Male\" & body_mass_g <= quantile(body_mass_g, 0.25, na.rm = TRUE) ~ \"smol_boi\",\n    sex == \"Female\" & body_mass_g <= quantile(body_mass_g, 0.25, na.rm = TRUE) ~ \"smol_gurl\",\n    TRUE ~ \"average\"\n  ))\n```\n:::\n\n\n> In this code:\n\n> We use the case_when() function to define the conditions based on sex and body mass to categorize the penguins.\n\n> For males with a body mass greater than the 75th percentile, they are categorized as \"chonky_boi.\" For females with a body mass greater than the 75th percentile, they are categorized as \"chonky_gurl.\"\n\n> For males with a body mass less than or equal to the 25th percentile, they are categorized as \"smol_boi.\" For females with a body mass less than or equal to the 25th percentile, they are also categorized as \"smol_gurl.\"\n\n> All other penguins are categorized as \"average.\"\n\n> We then use mutate() to create the new variable \"chonkiness\" based on these conditions.\n\n\nI will spare you the full adventure (my [chat history is here](https://chat.openai.com/share/769637ca-61bd-4ac5-9afb-d21381c5c99e) if you'd like to see) but after quite a bit of back-and-forth I realised that in the dataset \"male\" and \"female\" should both be lowercase and the code ChatGPT gave me had capitalised them both. **There is no substitute for knowing your data**.\n\nOnce I changed this, it worked.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Create a new variable based on conditions\npenguins <- penguins %>%\n  mutate(chonkiness = case_when(\n    sex == \"male\" & body_mass_g > quantile(body_mass_g, 0.75, na.rm = TRUE) ~ \"chonky_boi\",\n    sex == \"female\" & body_mass_g > quantile(body_mass_g, 0.75, na.rm = TRUE) ~ \"chonky_gurl\",\n    sex == \"male\" & body_mass_g <= quantile(body_mass_g, 0.25, na.rm = TRUE) ~ \"smol_boi\",\n    sex == \"female\" & body_mass_g <= quantile(body_mass_g, 0.25, na.rm = TRUE) ~ \"smol_gurl\",\n    TRUE ~ \"average\"\n  ))\n```\n:::\n\n\nCopilot used a median split and didn't try and create a third category, which is a questionable choice because the wording of the prompt really implies there needs to be something that the penguins are larger or smaller than. The code did work out of the box but remember the Jurassic Park rule: just because you can, doesn't mean you should.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Assuming 'penguins' is your dataframe and 'body_mass_g' is the variable for body mass\n# First, we need to define what we consider 'large' and 'small'. \n# For this example, let's assume penguins heavier than the median are 'large', \n# and those equal or lighter are 'small'.\n\n# Calculate the median body mass\nmedian_body_mass <- median(penguins$body_mass_g, na.rm = TRUE)\n\n# Create the new variable 'size_category'\npenguins <- penguins %>%\n  mutate(size_category = case_when(\n    body_mass_g > median_body_mass & sex == \"male\"   ~ \"chonky_boi\",\n    body_mass_g > median_body_mass & sex == \"female\" ~ \"chonky_gurl\",\n    body_mass_g <= median_body_mass & sex == \"male\"   ~ \"smol_boi\",\n    body_mass_g <= median_body_mass & sex == \"female\" ~ \"smol_gurl\",\n    TRUE ~ NA_character_  # for missing or other values\n  ))\n```\n:::\n\n\nThis code is a good example of why LLMs are inherently Luddites, even if they produce working code. When there's missing data, `case_when()` used to require you to specify the type of `NA`, in this case `NA_character`. This is very uninintuitive, particularly when you're trying to teach beginner coders (how can missing data have a type? is an entirely reasonable question) so I was delighted when it updated to remove this requirement. Additionally, `TRUE` was also changed to `default` which is a bit more readable. \n\nHowever, these changes are not reflected in Copilot's code suggestion because the training data is older than the most recent update to `case_when()`. This example is relatively minor but it's important to recognise that if you only use these kinds of AI to write code, you will very quickly be writing outdated code and using deprecated functions.\n\n## Activity 5: Further analysis\n\nTo give one last example, I entered the following prompt:\n\n> write me code to run a linear regression predicting body mass from sex. also include the code to perform necessary assumption tests using the easystats performance package.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(easystats)\n\n# Load the penguins dataset\ndata(\"penguins\")\n\n# Filter out rows with missing values in body mass or sex\npenguins <- penguins %>%\n  filter(!is.na(body_mass_g) & !is.na(sex))\n\n# Convert sex to lowercase\npenguins$sex <- tolower(penguins$sex)\n\n# Run linear regression predicting body mass from sex\nlm_model <- lm(body_mass_g ~ sex, data = penguins)\n\n# Display summary of the linear regression model\nsummary(lm_model)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n\nCall:\nlm(formula = body_mass_g ~ sex, data = penguins)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-1295.7  -595.7  -237.3   737.7  1754.3 \n\nCoefficients:\n            Estimate Std. Error t value Pr(>|t|)    \n(Intercept)  3862.27      56.83  67.963  < 2e-16 ***\nsexmale       683.41      80.01   8.542  4.9e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 730 on 331 degrees of freedom\nMultiple R-squared:  0.1806,\tAdjusted R-squared:  0.1781 \nF-statistic: 72.96 on 1 and 331 DF,  p-value: 4.897e-16\n```\n\n\n:::\n\n```{.r .cell-code}\n# Perform assumption tests using the performance package\ncheck_assumptions(lm_model)\n```\n\n::: {.cell-output .cell-output-error}\n\n```\nError in check_assumptions(lm_model): could not find function \"check_assumptions\"\n```\n\n\n:::\n:::\n\n\nChatGPT refuses to forget that in my conversation history there was an issue with the code not being lowercase, so it's now including functions to covert to lowercase, even though it's completely unnecessary. \n\nIt's also introduced an error. The function should be `check_model()` not `check_assumptions()`. I know this because I know the package. The further away you get from what you could do yourself, the harder it will be to fix any errors and check what it has gotten wrong.\n\n## Conclusions and musings\n\nA large part of coding is looking up examples of code you've used before and then tweaking it to your new use case. If you use AI like this, then it can really help speed you up by giving you access to boiler plate code you can adapt. I also find it very useful for those functions that just refuse to stick in my brain - every time I try to change the labels on a faceted `ggplot()` I get it wrong. These days I just go straight to ChatGPT.\n\nAdditionally, if what you're wanting to achieve is relatively simple - descriptive stats, basic plots, simple wrangling and cleaning - it can't go that far wrong and it's usually easy enough to spot when something is wrong.\n\nThese use cases work well because it's your expertise that drives what the AI does and reviewing and tweaking the output is built-in to these workflows. These are the AI skills that everyone is going to need. \n\nBut using code written by an AI that you don't understand is dangerous and irresponsible. Think of all the stupid errors it has made throughout this book, and then imagine someone using it to write code for their medical research. I dread to think of the impact that this abdication of responsibility and expertise is going to have (and will already be having). \n\nThis is a depressing way to end this book so I gave all three AIs the prompt:\n\n> Write me a fun piece of R code.\n\nChatGPT reassuring me comedy isn't going to be outsourced any time soon:\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Function to generate a random joke\ngenerate_joke <- function() {\n  jokes <- c(\n    \"Why don't scientists trust atoms? Because they make up everything!\",\n    \"I told my wife she was drawing her eyebrows too high. She looked surprised!\",\n    \"Why did the scarecrow win an award? Because he was outstanding in his field!\",\n    \"Why don't skeletons fight each other? They don't have the guts!\",\n    \"I'm reading a book on anti-gravity. It's impossible to put down!\"\n  )\n  return(sample(jokes, 1))\n}\n\n# Generate and print a random joke\ncat(\"Here's a random joke for you:\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nHere's a random joke for you:\n```\n\n\n:::\n\n```{.r .cell-code}\ncat(generate_joke(), \"\\n\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nWhy did the scarecrow win an award? Because he was outstanding in his field! \n```\n\n\n:::\n:::\n\n\nCopilot on its way to Vegas (ok I actually quite like this one):\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Simple Slot Machine in R\nset.seed(Sys.time()) # Seed for randomness based on current time\n\n# Function to spin the slot machine\nspin_slot_machine <- function() {\n  fruits <- c(\"🍒\", \"🍋\", \"🍊\", \"🍉\", \"🍇\", \"🍓\")\n  spin <- sample(fruits, size = 3, replace = TRUE)\n  cat(\"Spinning... You got:\", spin, \"\\n\")\n  \n  if (length(unique(spin)) == 1) {\n    cat(\"Congratulations! You won! 🎉\\n\")\n  } else {\n    cat(\"Try again! 🍀\\n\")\n  }\n}\n\n# Spin the slot machine\nspin_slot_machine()\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nSpinning... You got: 🍉 🍇 🍇 \nTry again! 🍀\n```\n\n\n:::\n:::\n\n\n::: {.callout-caution}\nThis book was written in Spring 2024 and should be considered a **living document**. The functionality and capability of AI is changing rapidly and the most recent advances may not reflect what is described in this book. Given the brave new world in which we now live, all constructive feedback and suggestions are welcome! If you have any feedback or suggestions, please provide it [via Forms](https://forms.office.com/Pages/ResponsePage.aspx?id=KVxybjp2UE-B8i4lTwEzyKAHhjrab3lLkx60RR1iKjNUM0VCMUUxUUFLMTdNM0JTS09PSDg2SFQ3US4u).\n:::\n",
    "supporting": [
      "06-writing_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}