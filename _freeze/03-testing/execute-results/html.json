{
  "hash": "c235558468b813c02d1768778afa94ea",
  "result": {
    "engine": "knitr",
    "markdown": "# Practice testing\n\nIn this chapter you'll learn how to use AI to test yourself. Practice testing, or [retrieval practice](https://www.learningscientists.org/blog/2016/6/23-1), is one of the most effective ways to consolidate learning, and it can be applied in several formats. This chapter was written with a data skills course in mind but it applies to any type of course. \n\nYou should have a specific week/chapter/lecture of your course to work with. For PsyTeachR courses like Applied Data Skills, we'll be using the Intended Learning Outcomes, functions used, and glossary to help instruct the AI. If you're not working through a course, or it's not a PsyTeachR course, it would be helpful to have a list of functions, concepts, or skills that you want to test. \n\nIt's very important that you use Copilot through your UofG account so that the course content you share is not used to train the model. You should also ensure that you have the consent of your lecturer to do this (if you're enrolled on Applied Data Skills, you have our consent to upload any course material to Copilot, but not to platforms like ChatGPT).\n\n## Question types\n\nThe first thing we're going to do is set-up several different prompts to create different types of practice questions. Which of these you end up using will depend on what you're trying to study. Different question formats test different aspects of your knowledge. Here’s what each type is for and what the AI will produce when you ask for it.\n\n### Questions that test recognition\n \n#### Multiple choice questions\n\nMCQs tests recognition and your ability to spot the best answer among distractors. Each item has a stem (the question) followed by four labelled options (A–D). Only one is correct. MQCs are efficient for covering a wide range of content, easy to self-check, and useful for practising discrimination between similar concepts.  However, they can also encourage recognition more than recall, and can sometimes be answered by guessing or test-wise strategies.\n\nExample:\n\n > Which of the following functions is used to create a scatterplot in R?\nA) geom_bar()\nB) geom_point()\nC) geom_boxplot()\nD) geom_histogram()\n\n#### True-or-false (TOF)\n\nTOF questions are a quick way to check factual accuracy and basic conceptual understanding and involve a single statement for you to judge as true or false. TOF questions are very quick to complete and good for simple checks of knowledge but have a high chance of guessing correctly (50%) and can oversimplify complex concepts and have limited diagnostic value.\n\nExample:\n\n> True or false? The mean() function in R returns the median of a numeric vector.\n\n#### Fill-in-the-blanks (coding)\n\nThese questions strengthen fluency with R syntax by making you supply missing functions or arguments but are easier than writing out the full code from scratch These questions involve a line of R code with a missing element. These questions support recall of key syntax and reduce cognitive load compared to writing from scratch but they can be too easy and may not transfer well to real coding tasks.\n\nExample:\n\n> ggplot(mtcars, aes(x = wt, y = mpg)) + ____()\n\n#### Fill-in-the-blanks (theory)\n\nSimilarly, for theory, FITB helps memorise key terms, definitions, or concepts without giving the full answer. These questions will be a sentence with 1–2 blanks, sometimes with hints. FIT is good for reinforcing vocabulary and key concepts and are quick to create and practise. But, there is a risk of rote memorisation without deeper understanding and answers may sometimes be ambiguous.\n\n\nExample:\n\n> “A variable that can take on any value between two points is called a ______.”\n\n### Questions that test production\n\n#### Short-answer-questions\n\nSAQs require you to recall and explain in your own words, building deeper understanding. These are focused, open questions that should be answered in <100 words. SAQs promote active recall and deeper processing and are flexible enough to test conceptual understanding. However, they are harder to self-mark and may be more time-consuming to generate and answer.\n\nExample:\n\n> Explain the difference between a categorical and a continuous variable.\n\n#### Coding problems\n\nCoding problems let you apply your skills to solve a real task, similar to authentic assessments. They usually involve a short programming challenge using a real dataset. Coding problems closely mimic real-world problem solving, encourage transfer of knowledge, and consolidate multiple skills at once. The higher difficulty can be discouraging for beginners and they may be harder to self-assess without feedback.\n\nExample:\n\n> “Using the penguins dataset, create a boxplot of body mass grouped by species.”\n\n\n#### Error mode\n\nError mode is debugging practice which builds resilience and problem-solving skills, and teaches you to spot common mistakes. It involves a runnable piece of R code that contains one plausible error. Error mode develops error-detection and debugging skills and mirrors real-world coding experience. However, it can be frustrating for novices and requires baseline knowledge to be effective.\n\nExample:\n\n> ggplot(mtcars, aes(x = wt, y = mpg))  \n  geom_point()\n  \n> (Error: missing + before geom_point())\n\n## Practical tip: Built-in datasets\n\nWhen practising coding questions with AI, you need a dataset you can actually run code on. A common problem with AI-generated exercises is that it invents datasets or column names that do not exist, which makes it impossible to test your solution. One way to avoid this is to rely on built-in datasets.\n\nBuilt-in datasets in R are sample datasets that come pre-loaded with the software or with specific packages. They cover a variety of domains (e.g., cars, gemstones, movies, penguins) and are designed to help you practise data manipulation, analysis, and visualisation without importing external files.\n\nYou can get a full list of available datasets by running `data()` in the console. Base R provides some, and additional packages (e.g., `tidyverse`, `palmerpenguins`) add more. Remember that a package must be loaded before its datasets are accessible.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# see list of datasets\ndata()\n\n# load tidyverse to get access to extra sets\nlibrary(tidyverse)\n\n# see list of datasets expanded to include tidyverse ones\ndata()\n\n# load in dataset to environment so it can be used\ndata(\"starwars\")\n```\n:::\n\n\n## Question prompt\n\nWe will now design a prompt that will set-up Copilot to give you different types of questions.\n\n::: {.callout-note}\n\n## Activity 1\n\nEdit the below template to suit your needs. For example, change the role, the sources you're going to use, and if relevant, the datasets it should draw on in constraints. You can also change the other details although most of them should work for you without any changes.\n\n:::\n\n::: {.callout-tip}\n## Template prompt to copy and paste into Copilot\n\nRole: You are a strict practice-testing tutor for second year undergraduate students learning R.\n\nSources: I will paste Intended Learning Outcomes (ILOs), function lists, and glossary terms from my course Applied Data Skills. \n\n\nProtocol:\n\n1. Ask one question at a time of the requested type.\n\n2. Do not reveal the answer until I reply.\n\n3. Do not give the answer away in the suggested follow-up prompts. For example, for MCQs, give suggested prompts for each answer option, not just the correct answer.\n\n4. After I answer, mark it, then give a short explanation (2–4 sentences).\n\n\nConstraints:\n\n1. All questions must align to ILOs and be challenging but fair.\n\n2. For coding questions, avoid imaginary datasets or columns. Instead, use only use the following datasets: starwars, diamonds, penguins.\n\n\nControls I will use:\n\n1. type: ... (see list below)\n\n2. calibrate: harder | easier\n\n\nQuestion Types (with rules):\n\n1. MCQ (Multiple choice) – 4 options (A–D), 1 correct. Plausible distractors. After marking, explain each option briefly.\n\n2. TOF (True/False) – One statement; avoid trivially true/false. After marking, if false, rewrite as a correct statement.\n\n3. SAQ (Short answer) – Ask about one concept. I should be able to answer in <100 words. After marking, provide a 2–3 point ideal outline.\n\n4. FITB_code (Fill-in-the-blank: coding) – One line of R with a missing function/argument. Must run on an approved dataset. State expected output shape (e.g., “tibble: 3 × 2”).\n\n5. FITB_theory (Fill-in-the-blank: theory) – A statement with up to 2 blanks. Give part-of-speech hints (e.g., [noun]).\n\n6. CP (Coding problem) – Minimal complete example using an approved dataset. Specify columns that exist. Do not invent data.\n\n7. EM (Error mode) – Provide a minimal reproducible example that fails using a specified dataset. The code should include the call to load the dataset and any required packages. Include exactly one plausible beginner-level error but give no hint as to what the error is. State intended outcome. Hold back the fix until after I attempt a solution. Then reveal the correction and what it teaches.\n\nFirst action: Confirm readiness and ask me to provide the sources.\n\n:::\n\n\n## Sources\n\nOnce you have set up this prompt, you can give it the sources to work from—for example, copy and paste the ILOs, a list of functions, and/or key terms you want it to quiz you on. Do not worry about formatting: just paste the text in as it is.\n\n::: {.callout-note}\n## Activity 2\n\nAdd in your sources for the content you want to study.\n\n:::\n\n\n## Test yourself\n\nYou can now ask it to generate questions for you by typing `mcq` or `coding problem`. If the questions seem too easy or hard, you can adjust them by using `calibrate: easier` or `calibrate: harder`\n\n:::{.callout-note}\n\n## Activity 3\n\nAsk if for one question of each type and work through them. Reflect on whether they are challenging enough, if they align with your course content, and how they might differ from practice questions you have been given by a human.\n\n:::\n\n::: {.callout-caution}\n\nThe suggested follow-up prompts in Copilot sometimes indicate what the correct answer is, particularly for closed-questions like MCQs. I've not found a reliable way to stop it from doing this - let me know if you crack it. \n\n:::\n\n## Be critical\n\nEffective practice testing is not just about answering more questions; it is about engaging the cognitive processes that drive learning  -again [metacognition](https://www.learningscientists.org/blog/2024/5/23-1?), [desirable difficulties](https://bjorklab.psych.ucla.edu/wp-content/uploads/sites/13/2016/04/EBjork_RBjork_2011.pdf) and [self-explanation](https://www.learningscientists.org/blog/2020/2/20-1) are key. The questions AI generates can be useful, but they can also introduce illusions of fluency and miscalibration. \n\n### Multiple-choice / TOF / SAQ questions\n\n-   Sometimes the answers are simply wrong. If you challenge the AI, it will usually correct itself—but it will also agree with you if you claim an answer is wrong when it is not. The risk is that you accidentally encode misinformation. Remember: AI does not “know” anything; it is a sophisticated pattern-matcher, not a reliable authority.\n-   It may generate questions about functions or concepts not covered in your course, which can cause confusion and unnecessary anxiety.\n-   Occasionally it poses a question with multiple correct answers without making this clear, which is frustrating.\n-   It may overemphasise particular topics or functions unless you explicitly direct it to vary the focus.\n\n### Coding problems\n\n- The examples are not always reproducible. For instance, it might assume the existence of a dataset with variables called “number” and “price” but provide no such dataset, making it impossible to run the code. You can still attempt the problem, but this adds extra difficulty, especially for beginners.\n- It sometimes uses functions or approaches you have not been taught, such as defaulting to Base R instead of tidyverse.\n\n### Error mode\n\n- In the first iteration of this book, AI could not create error mode problems - the code either ran fine or the errors were so stupid and obvious it was of no educational benefit. Models have improved, and they can now generate plausible errors, which is an interesting sign of progress.\n- Even so, AI often hints heavily at the error or simply tells you the answer, which reduces the learning benefit.\n- Whilst it can now generate plausible errors, it doesn't have the benefit of years of teaching experience. When we design error mode questions, they're based on our knowledge of what students frequently get wrong so AI error mode isn't always as targeted and therefore as useful as questions written by an expert educator. Maybe our jobs are safe for a little while longer.\n\n::: {.callout-tip}\n\n## Key Takeaways\n\n1. Regular retrieval strengthens memory more than re-reading or highlighting.\n\n2. Choose the right format for questions: MCQs/TOF are good for recognition and breadth but weaker for recall. SAQs/FITB ae better for active recall and vocabulary but risk of rote learning. Coding problems/Error mode are closest to authentic tasks, but require more effort and background knowledge.\n\n3. Paste ILOs, function lists, or glossaries into Copilot to ensure it's specific to what you are studying. Avoid imaginary datasets—stick to built-in ones like mtcars, penguins, or iris.\n\n4. AI outputs can be wrong, misleading, or oddly focused. Correcting them is part of the learning process and strengthens metacognition.\n\n5. AI is a tool, not a teacher. It lacks expert judgement about what learners typically get wrong. Use it to supplement, not replace, structured practice and seeking help from your lectuers and tutors.\n\n:::\n\n## Test yourself\n\n\n::: {.cell}\n\n:::\n\n\n\n1. Why does practice testing improve long-term retention?\n\n<select class='webex-select'><option value='blank'></option><option value=''>Because it increases exposure to learning materials without requiring recall</option><option value=''>Because it allows learners to memorise correct answers through repetition alone</option><option value='answer'>Because it strengthens retrieval pathways by requiring effortful recall and feedback</option><option value=''>Because it reduces cognitive load by simplifying the learning task</option></select>\n\n>\n\n2. What key difference distinguishes multiple-choice questions (MCQs) from short-answer questions (SAQs)?\n\n<select class='webex-select'><option value='blank'></option><option value=''>MCQs and SAQs both primarily measure recognition memory</option><option value='answer'>MCQs test recognition of correct information, while SAQs test recall and explanation in your own words</option><option value=''>SAQs are easier to write but harder to mark objectively than MCQs</option><option value=''>MCQs promote deeper understanding because they include distractors</option></select>\n\n>\n\n3. How does regular self-testing affect a learner’s metacognitive accuracy?\n\n<div class='webex-radiogroup' id='radio_FLLBAECAZX'><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_FLLBAECAZX\" value=\"\"></input> <span>Frequent testing reduces metacognition because learners focus on results instead of process</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_FLLBAECAZX\" value=\"answer\"></input> <span>Frequent testing improves metacognitive calibration by revealing what learners actually know versus what they think they know</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_FLLBAECAZX\" value=\"\"></input> <span>Self-testing has no measurable effect on metacognitive accuracy</span></label><label><input type=\"radio\" autocomplete=\"off\" name=\"radio_FLLBAECAZX\" value=\"\"></input> <span>Testing reduces motivation and therefore limits reflection on learning progress</span></label></div>\n\n\n>\n\n4. AI-generated questions can create <input class='webex-solveme nospaces ignorecase' size='20' data-answer='[\"illusions\",\"illusions of fluency\"]'/> of fluency that make learners overestimate their understanding.\n\n>\n\n5. What strategy makes AI-generated practice testing most effective?\n<select class='webex-select'><option value='blank'></option><option value=''>Doing as many questions as possible in one long session</option><option value=''>Focusing only on questions that feel easy to boost confidence</option><option value='answer'>Spacing practice over time, reflecting on feedback, and verifying answers using reliable course materials</option><option value=''>Allowing AI to correct all mistakes automatically to reduce frustration</option></select>\n\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}